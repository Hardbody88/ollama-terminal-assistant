# Ollama Terminal Assistant

A Python command-line interface (CLI) that acts as an AI-powered terminal assistant. It uses a locally running Ollama instance to understand your natural language requests, translates them into one or more shell commands, asks for your confirmation before executing *each* command, runs the confirmed commands, and feeds the results back into the conversation context for more accurate follow-up requests.

## Features

*   Natural language interaction with your terminal.
*   Leverages local Ollama models (privacy-focused).
*   OS detection for context-aware command generation.
*   Structured JSON communication with Ollama for increased reliability.
*   Handles single or multiple sequential commands per user request.
*   **Crucial safety step:** Requires explicit user confirmation (Press Enter) before executing *each* suggested command. (`N` to skip).
*   Provides command explanations generated by the AI.
*   Displays command output (stdout/stderr) clearly.
*   Maintains conversation history, including command results, for contextual follow-up.
*   Configurable Ollama model and URL via environment variables.
*   Enhanced terminal output using the `rich` library (optional but recommended).

## Prerequisites

*   **Ollama:** Must be installed and running locally. Download from [ollama.com](https://ollama.com/).
*   **Ollama Model:** A suitable instruction-following model needs to be downloaded. Run `ollama list` to see available models.
    *   **Recommended:** `qwen3:4b`, `deepcoder:1.5b` or larger variants (04/2025).
    *   Get a model using `ollama run qwen3:4b` (replace model name as needed).
    *   *Note:* Smaller models may struggle to consistently follow the required JSON format.
*   **Python:** Version 3.7+ recommended.
*   **pip:** Python package installer (usually comes with Python).

## Configuration

The script uses environment variables for configuration (optional):

*   `OLLAMA_MODEL`: Sets the Ollama model to use. Defaults to `qwen3:4b` (or whatever is set in the script).
    ```bash
    export OLLAMA_MODEL="qwen3:4b"
    ```
*   `OLLAMA_BASE_URL`: Sets the base URL for your Ollama instance if it's not running on `http://localhost:11434`.
    ```bash
    export OLLAMA_BASE_URL="http://192.168.1.100:11434"
    ```

You can set these in your shell profile (e.g., `.bashrc`, `.zshrc`) for persistence or just before running the script.

## Usage

1.  **Start Ollama:** Ensure the Ollama application or service is running.
2.  **Run the script:**
    ```bash
    python ollama_terminal.py
    ```
3.  **Interact:**
    *   Type your request in natural language at the `You:` prompt.
    *   The script will query Ollama and display the proposed command(s) and an explanation.
    *   For **each** proposed command, you will be prompted: `Execute command X/Y? [Enter=Yes, N=No]:`
        *   Press `Enter` to execute the currently displayed command.
        *   Type `n` (or `N`) and press `Enter` to **skip** the currently displayed command.
        *   Any other input will also skip the command for safety.
    *   The command's output (stdout/stderr) or a skip notification will be shown.
    *   The results (or skip status) are added to the conversation history for Ollama's context.
4.  **Exit:** Type `exit` or `quit` and press Enter, or use `Ctrl+C`.

## How It Works

1.  **OS Detection:** The script determines the host operating system for better context.
2.  **System Prompt:** Sends a detailed system prompt to Ollama, including the OS info and instructions to respond in a specific JSON format (`{"commands": ["cmd1", ...], "explanation": "..."}`).
3.  **API Call:** Sends the user's request along with the conversation history to the Ollama API (`/api/chat`).
4.  **JSON Parsing:** Parses the expected JSON response from Ollama.
5.  **Command Iteration:** Loops through the list of commands received in the JSON.
6.  **Confirmation Loop:** For each command, it prompts the user for confirmation (`Enter` or `N`).
7.  **Execution:** If confirmed, executes the command using `subprocess.run(shell=True, ...)`.
8.  **Feedback:** Captures stdout, stderr, and the exit code. This result (or a skip notification) is added back into the chat history as a user message, informing Ollama of the outcome for subsequent requests.

## Security Considerations

This script uses `subprocess.run(shell=True, ...)` to execute commands suggested by the AI **after** your explicit confirmation for *each step*.

*   **Confirmation is Key:** The primary safety mechanism is your review and confirmation before *any* command is run.
*   **`shell=True` Risk:** Using `shell=True` allows complex commands (like pipes and redirects) suggested by the AI to be executed directly by the system's shell. While convenient, it carries inherent risks if a malicious or destructive command were ever mistakenly confirmed.
*   **Review Carefully:** **Always carefully review the command proposed by the assistant before pressing Enter.** Understand the potential impact of any command, especially those involving `rm`, `mv`, `dd`, `sudo`, or complex scripting elements. If unsure, type `N` to skip.
